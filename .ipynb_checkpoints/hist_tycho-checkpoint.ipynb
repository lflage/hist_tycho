{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Tycho\n",
    "\n",
    "Notebook criado para tratar os dados do corpus histórico do português brasileiros Tycho Brahe\n",
    "O objetivo deste notebook é tratar o corpus para gerar um modelos de Word Embeddings para diferentes gerações e, posteriormente, criar uma representação gráfica da mudança semântica para o português brasileiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importanto bibliotecas utilizadas\n",
    "import os, shutil, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biblioteca criada para tratar os dados\n",
    "import corpus_man as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = cm.spacy_loader()\n",
    "nlp.Defaults.stop_words |= {\"character\",'comment','title','author','language',}\n",
    "nlp.disable_pipes('ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrando a remoção do Reconhecimento de Entidades Nomeadas para agilizar o processo\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Preparação do Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removendo os cabeçalhos dos arquivos .txt, transformando os tokens em lowercase e retirando stopwords e acentos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separação em sentenças"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./txt/sentencized'):\n",
    "    os.makedirs('./txt/sentencized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"./txt/original\", topdown=False):\n",
    "    for name in files:\n",
    "        with open(os.path.join(root,name),'r') as file:\n",
    "            txt = file.read()\n",
    "            txt = re.split(r'\\[END HEADER\\]\\W+',txt)[1]\n",
    "            doc = nlp(txt)\n",
    "            with open('./txt/sentencized/'+ name, 'w') as sentencized:\n",
    "                for sent in doc.sents:\n",
    "                    sentencized.write(str(sent))\n",
    "                    sentencized.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de quebras de linhas em excesso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematização, remoção de acentos e lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"./txt/sentencized\", topdown=False):\n",
    "    for name in files:\n",
    "        with open(os.path.join(root,name),'r') as file:\n",
    "            with open('./txt/lematized_lookup/'+ name, 'w') as processed:\n",
    "                for line in file.readlines():\n",
    "                    if line.strip():\n",
    "                        processed.write(cm.pre_process(line))\n",
    "                        processed.write('\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificando os arquivos\n",
    "\n",
    "O próximo passo é ordenar e agrupar os arquivos de texto de acordo com a metainformação \"Period by Birthdate\", que corresponde ao período do texto de acordo com a data de nascimento do autor. Os textos foram manualmente classificados de acordo com seu pertencimento a cada período como definido por \"Bechara\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_dict = {}\n",
    "with open('periodos_tycho.csv', mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    cor_dict = {row[1]:row[0] for row in reader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variável dir_path contém o caminho até a pasta de destino onde os textos serão separados em períodos diferentes.\n",
    "Para a criação de diferentes versões de modelos somente esse caminho foi alterado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'txt_sorts/v2'\n",
    "if not os.path.exists(dir_path):\n",
    "    os.makedirs(dir_path)\n",
    "\n",
    "if not os.path.exists(dir_path + '/p1'):\n",
    "    os.makedirs(dir_path + '/p1')\n",
    "if not os.path.exists(dir_path + '/p2'):\n",
    "    os.makedirs(dir_path + '/p2')\n",
    "if not os.path.exists(dir_path + '/p3'):\n",
    "    os.makedirs(dir_path + '/p3')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(\"./txt/processed\", topdown=False):\n",
    "    for file in files:\n",
    "        if cor_dict[file] == 'arcmed':\n",
    "            source = os.path.join(root,file)\n",
    "            target = dir_path + '/p1'\n",
    "            shutil.copy(source,target)\n",
    "        if cor_dict[file] == 'mod':\n",
    "            source = os.path.join(root,file)\n",
    "            target = dir_path + '/p2'\n",
    "            shutil.copy(source,target)\n",
    "        if cor_dict[file] == 'cont1' or cor_dict[file] == 'cont2':\n",
    "            source = os.path.join(root,file)\n",
    "            target = dir_path + '/p3'\n",
    "            shutil.copy(source,target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criando um dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumps_path = dir_path+'/dumps'\n",
    "\n",
    "if not os.path.exists(dumps_path):\n",
    "    os.makedirs(dumps_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dumps_path+'/dump1', 'w') as dump:\n",
    "    for root, dirs, files in os.walk(dir_path+'/p1', topdown=False):\n",
    "        for file in files:\n",
    "            with open(os.path.join(root,file),'r') as text:\n",
    "                for content in text.readlines():\n",
    "                    dump.write(content)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dumps_path+'/dump2', 'w') as dump:\n",
    "    for root, dirs, files in os.walk(dir_path+'/p2', topdown=False):\n",
    "        for file in files:\n",
    "            with open(os.path.join(root,file),'r') as text:\n",
    "                for content in text.readlines():\n",
    "                    dump.write(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(dumps_path+'/dump3', 'w') as dump:\n",
    "    for root, dirs, files in os.walk(dir_path+'/p3', topdown=False):\n",
    "        for file in files:\n",
    "            with open(os.path.join(root,file),'r') as text:\n",
    "                for content in text.readlines():\n",
    "                    dump.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento dos Modelos\n",
    "\n",
    "Os modelos foram treinados serparadamente usando o código fornecido por [Mikolov et al ](https://github.com/tmikolov/word2vec).\n",
    "\n",
    "Os hiperparâmetros utilizados foram:  \n",
    "-cbow 1  \n",
    "-size 300   \n",
    "-window 8   \n",
    "-negative 25   \n",
    "-hs 0  \n",
    "-sample 1e-4   \n",
    "-threads 20   \n",
    "-binary 1   \n",
    "-iter 25   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando modelos do Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os modelos foram carregados um a um separadamente e em seguida plotados utilizando a biblioteca plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(\n",
    "    './txt_sorts/v2/vectors/vectors_dump1.bin',\n",
    "    binary=True)\n",
    "# dump = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(\n",
    "#     './txt_sorts/v2/vectors/vectors_dump2.bin',\n",
    "#     binary=True)\n",
    "# dump = gensim.models.keyedvectors.KeyedVectors.load_word2vec_format(\n",
    "#     './txt_sorts/v2/vectors/vectors_dump3.bin',\n",
    "#     binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import IncrementalPCA    # inital reduction\n",
    "from sklearn.manifold import TSNE                   # final reduction\n",
    "import numpy as np                                  # array handling\n",
    "\n",
    "\n",
    "def reduce_dimensions(model):\n",
    "    '''Função para reduzir o número de dimensões dos vetores de palavras a 2.\n",
    "    model : gensim.Keyedvector\n",
    "    '''\n",
    "    num_dimensions = 2  # final num dimensions (2D, 3D, etc)\n",
    "\n",
    "    # extract the words & their vectors, as numpy arrays\n",
    "    vectors = np.asarray(model.vectors)\n",
    "    labels = np.asarray(model.index_to_key)  # fixed-width numpy strings\n",
    "\n",
    "    # reduce using t-SNE\n",
    "    tsne = TSNE(n_components=num_dimensions, random_state=0)\n",
    "    vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "    x_vals = [v[0] for v in vectors]\n",
    "    y_vals = [v[1] for v in vectors]\n",
    "    return x_vals, y_vals, labels\n",
    "\n",
    "def my_labels(x_val, y_val, labels, my_words):\n",
    "    '''Função para recuperar os índices de palavras a serem destacadas na visualização\n",
    "    my_words : list of strings\n",
    "    labels : np.arrey\n",
    "    y_val : list\n",
    "    x_val : list\n",
    "    '''\n",
    "    inxs = [np.where(labels == i)[0][0] for i in my_words]    \n",
    "    return ([x_val[i] for i in inxs], [y_val[i] for i in inxs])\n",
    "\n",
    "# Usando a função para obter valores de x, y e os rótulos para as coordenadas\n",
    "x_vals, y_vals, labels = reduce_dimensions(dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_words = ['deus','mulher','homem','pai','mae','terra']\n",
    "\n",
    "my_x,my_y = my_labels(x_vals,y_vals,labels, my_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_with_plotly(x_vals, y_vals, labels, plot_in_notebook=True):\n",
    "    from plotly.offline import init_notebook_mode, iplot, plot\n",
    "    import plotly.graph_objs as go\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scattergl(x=x_vals, y=y_vals, mode='text', text=labels,\n",
    "                              ))\n",
    "    fig.add_trace(go.Scattergl(x=my_x,y=my_y,mode='text',text=my_words,\n",
    "                               marker_color='red'))\n",
    "    fig.for_each_trace(lambda t: t.update(textfont_color=t.marker.color, textposition='top center'))\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_with_plotly(x_vals, y_vals, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
